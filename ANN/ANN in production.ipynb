{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f5a0533",
   "metadata": {},
   "source": [
    "<u>Unresolved Questions</u>\n",
    "\n",
    "- <i>Derive an equation of backpropogation in multiclass classification</i>\n",
    "- <i>When to use `categorical cross-entropy` and when to use `sparse categorical cross-entropy`</i>\n",
    "- <i>Why GPU's are preferred to train neural networks over CPU's</i>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49331064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as transforms # Transforming images\n",
    "\n",
    "from typing import Tuple, Optional, Callable\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bf8d4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa6a0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class with Features\n",
    "class FashionMNISTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class with transforms, validation, and error handling\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 features: torch.Tensor, \n",
    "                 labels: torch.Tensor, \n",
    "                 transform: Optional[Callable] = None, # Callable means it act as a function\n",
    "                 normalize: bool = True,\n",
    "                 mode: str = 'train') -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: Feature tensor\n",
    "            labels: Label tensor  \n",
    "            transform: Optional transform to be applied on features\n",
    "            normalize: Whether to normalize pixel values to [0,1]\n",
    "            mode: 'train', 'val', or 'test' - affects data augmentation\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input validation\n",
    "        if features.shape[0] != labels.shape[0]:\n",
    "            raise ValueError(f\"Features and labels must have same length. \"\n",
    "                           f\"Got {features.shape} and {labels.shape}\")\n",
    "        \n",
    "        self.features = features\n",
    "        self.labels = labels.long()  # Ensure labels are long type for CrossEntropy (Otherwise error)\n",
    "        self.normalize = normalize\n",
    "        self.mode = mode\n",
    "        \n",
    "        # Set default transforms if none provided\n",
    "        if transform is None:\n",
    "            self.transform = self._get_default_transforms() # Our own transformation function\n",
    "        else:\n",
    "            self.transform = transform # Function variable is passed which is callable\n",
    "            \n",
    "        # Normalize pixel values to [0,1] if requested\n",
    "        if self.normalize:\n",
    "            self.features = self.features.float() / 255.0\n",
    "            \n",
    "        print(f\"Dataset created: {self.mode} mode, {len(self)} samples\")\n",
    "        \n",
    "    def _get_default_transforms(self): # <- private method\n",
    "        \"\"\"Get default transforms based on mode\"\"\"\n",
    "        if self.mode == 'train': # Transformation only applied while training\n",
    "            # Data augmentation for training\n",
    "            return transforms.Compose([\n",
    "                transforms.RandomRotation(degrees=10),\n",
    "                transforms.RandomHorizontalFlip(p=0.1),  # Less common for fashion items\n",
    "                transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "                transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1]\n",
    "            ])\n",
    "        else:\n",
    "            # Only normalization for val/test\n",
    "            return transforms.Compose([\n",
    "                transforms.Normalize(mean=[0.5], std=[0.5]) # Normalization is a preprocessing so we need to apply it on test data as well.\n",
    "            ])\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.features.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Get item with error handling\"\"\"\n",
    "        try:\n",
    "            feature = self.features[index]\n",
    "            label = self.labels[index]\n",
    "            \n",
    "            # Apply transforms if any\n",
    "            if self.transform:\n",
    "                feature = self.transform(feature)\n",
    "                \n",
    "            return feature, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading index {index}: {e}\")\n",
    "            # Return a default sample in case of error\n",
    "            return self.features[0], self.labels[0]\n",
    "    \n",
    "    def get_class_distribution(self):\n",
    "        \"\"\"Get class distribution for analysis\"\"\"\n",
    "        unique, counts = torch.unique(self.labels, return_counts=True) # Also calculates the frequency of each class\n",
    "        return dict(zip(unique.tolist(), counts.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "769a2e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading and Splitting Pipeline\n",
    "class DataManager:\n",
    "    \"\"\"\n",
    "    Data management class for loading and splitting\n",
    "    \"\"\"\n",
    "    def __init__(self, train_csv_path: str, test_csv_path: str, random_state: int = 42):\n",
    "        self.train_csv_path = train_csv_path\n",
    "        self.test_csv_path = test_csv_path\n",
    "        self.random_state = random_state\n",
    "        self.class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "        \n",
    "    def load_raw_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Load raw CSV data with error handling\"\"\"\n",
    "        try:\n",
    "            train_df = pd.read_csv(self.train_csv_path)\n",
    "            test_df = pd.read_csv(self.test_csv_path)\n",
    "            \n",
    "            print(f\"Loaded training data: {train_df.shape}\")\n",
    "            print(f\"Loaded test data: {test_df.shape}\")\n",
    "            \n",
    "            # Validate data\n",
    "            self._validate_data(train_df, test_df)\n",
    "            \n",
    "            return train_df, test_df\n",
    "        except Exception as e:\n",
    "            raise FileNotFoundError(f\"Error loading data: {e}\")\n",
    "    \n",
    "    def _validate_data(self, train_df: pd.DataFrame, test_df: pd.DataFrame):\n",
    "        \"\"\"Validate loaded data\"\"\"\n",
    "        # Check for missing values\n",
    "        if train_df.isnull().any().any():\n",
    "            warnings.warn(\"Training data contains missing values\")\n",
    "        if test_df.isnull().any().any():\n",
    "            warnings.warn(\"Test data contains missing values\")\n",
    "            \n",
    "        # Check label range\n",
    "        train_labels = train_df.iloc[:, 0]\n",
    "        test_labels = test_df.iloc[:, 0]\n",
    "        \n",
    "        if train_labels.min() < 0 or train_labels.max() > 9:\n",
    "            raise ValueError(\"Training labels should be between 0-9\") # Specific to this dataset\n",
    "        if test_labels.min() < 0 or test_labels.max() > 9:\n",
    "            raise ValueError(\"Test labels should be between 0-9\")\n",
    "    \n",
    "    def create_stratified_splits(self, \n",
    "                               train_df: pd.DataFrame,\n",
    "                               val_size: float = 0.2,\n",
    "                               stratify: bool = True) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Create stratified train/validation split\n",
    "        \n",
    "        Args:\n",
    "            train_df: Training dataframe\n",
    "            val_size: Proportion for validation set\n",
    "            stratify: Whether to maintain class distribution\n",
    "        \"\"\"\n",
    "        features = train_df.iloc[:, 1:].values\n",
    "        labels = train_df.iloc[:, 0].values\n",
    "        \n",
    "        if stratify:\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                features, labels,\n",
    "                test_size=val_size,\n",
    "                stratify=labels,\n",
    "                random_state=self.random_state\n",
    "            )\n",
    "        else:\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                features, labels,\n",
    "                test_size=val_size,\n",
    "                random_state=self.random_state\n",
    "            )\n",
    "        \n",
    "        # Convert back to DataFrames\n",
    "        train_split = pd.DataFrame(np.column_stack([y_train, X_train]))\n",
    "        val_split = pd.DataFrame(np.column_stack([y_val, X_val]))\n",
    "        \n",
    "        print(f\"Training set: {train_split.shape}\")\n",
    "        print(f\"Validation set: {val_split.shape}\")\n",
    "        \n",
    "        return train_split, val_split\n",
    "    \n",
    "    def prepare_tensors(self, df: pd.DataFrame) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Convert DataFrame to tensors with proper shapes\"\"\"\n",
    "        # Separate features and labels\n",
    "        labels = torch.tensor(df.iloc[:, 0].values, dtype=torch.long)\n",
    "        features = torch.tensor(df.iloc[:, 1:].values, dtype=torch.float32)\n",
    "        \n",
    "        # Reshape features to image format [N, 1, 28, 28]\n",
    "        features = features.reshape(-1, 1, 28, 28)\n",
    "        \n",
    "        return features, labels\n",
    "    \n",
    "    # OPTIONAL\n",
    "    def get_data_statistics(self, features: torch.Tensor, labels: torch.Tensor, name: str):\n",
    "        \"\"\"Print dataset statistics\"\"\"\n",
    "        print(f\"\\n{name} Dataset Statistics:\")\n",
    "        print(f\"Features shape: {features.shape}\")\n",
    "        print(f\"Labels shape: {labels.shape}\")\n",
    "        print(f\"Feature range: [{features.min():.2f}, {features.max():.2f}]\")\n",
    "        \n",
    "        # Class distribution\n",
    "        unique, counts = torch.unique(labels, return_counts=True)\n",
    "        print(\"Class distribution:\")\n",
    "        for class_idx, count in zip(unique, counts):\n",
    "            class_name = self.class_names[class_idx]\n",
    "            print(f\"  {class_idx} ({class_name}): {count} samples ({count/len(labels)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f02c5d85",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Error loading data: [Errno 2] No such file or directory: 'fashion-mnist_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mDataManager.load_raw_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     train_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_csv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     test_df = pd.read_csv(\u001b[38;5;28mself\u001b[39m.test_csv_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programming\\Deep Learning\\myenv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programming\\Deep Learning\\myenv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programming\\Deep Learning\\myenv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programming\\Deep Learning\\myenv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programming\\Deep Learning\\myenv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    871\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m     \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m     handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m        \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m     \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'fashion-mnist_train.csv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      2\u001b[39m data_manager = DataManager(\n\u001b[32m      3\u001b[39m     train_csv_path=\u001b[33m\"\u001b[39m\u001b[33mfashion-mnist_train.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     test_csv_path=\u001b[33m\"\u001b[39m\u001b[33mfashion-mnist_test.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     random_state=\u001b[32m42\u001b[39m\n\u001b[32m      6\u001b[39m )\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Load raw data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m train_df, test_df = \u001b[43mdata_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_raw_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Create stratified train/validation split\u001b[39;00m\n\u001b[32m     12\u001b[39m train_split, val_split = data_manager.create_stratified_splits(\n\u001b[32m     13\u001b[39m     train_df, \n\u001b[32m     14\u001b[39m     val_size=\u001b[32m0.15\u001b[39m,  \u001b[38;5;66;03m# 15% for validation\u001b[39;00m\n\u001b[32m     15\u001b[39m     stratify=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     16\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mDataManager.load_raw_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m train_df, test_df\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError loading data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Error loading data: [Errno 2] No such file or directory: 'fashion-mnist_train.csv'"
     ]
    }
   ],
   "source": [
    "# Initialize Data Manager and Load Data\n",
    "data_manager = DataManager(\n",
    "    train_csv_path=\"fashion-mnist_train.csv\",\n",
    "    test_csv_path=\"fashion-mnist_test.csv\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Load raw data\n",
    "train_df, test_df = data_manager.load_raw_data()\n",
    "\n",
    "# Create stratified train/validation split\n",
    "train_split, val_split = data_manager.create_stratified_splits(\n",
    "    train_df, \n",
    "    val_size=0.15,  # 15% for validation\n",
    "    stratify=True\n",
    ")\n",
    "\n",
    "# Prepare tensors\n",
    "train_features, train_labels = data_manager.prepare_tensors(train_split)\n",
    "val_features, val_labels = data_manager.prepare_tensors(val_split)\n",
    "test_features, test_labels = data_manager.prepare_tensors(test_df)\n",
    "\n",
    "# Print statistics\n",
    "data_manager.get_data_statistics(train_features, train_labels, \"Training\")\n",
    "data_manager.get_data_statistics(val_features, val_labels, \"Validation\") \n",
    "data_manager.get_data_statistics(test_features, test_labels, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a1dc263f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created: train mode, 51000 samples\n",
      "Dataset created: val mode, 9000 samples\n",
      "Dataset created: test mode, 10000 samples\n",
      "\n",
      "Class distributions:\n",
      "Training: {0: 5100, 1: 5100, 2: 5100, 3: 5100, 4: 5100, 5: 5100, 6: 5100, 7: 5100, 8: 5100, 9: 5100}\n",
      "Validation: {0: 900, 1: 900, 2: 900, 3: 900, 4: 900, 5: 900, 6: 900, 7: 900, 8: 900, 9: 900}\n",
      "Test: {0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000, 6: 1000, 7: 1000, 8: 1000, 9: 1000}\n"
     ]
    }
   ],
   "source": [
    "# Create Industrial-Grade Datasets\n",
    "# Training dataset with augmentation\n",
    "train_dataset = FashionMNISTDataset(\n",
    "    features=train_features,\n",
    "    labels=train_labels,\n",
    "    mode='train',\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "# Validation dataset without augmentation\n",
    "val_dataset = FashionMNISTDataset(\n",
    "    features=val_features,\n",
    "    labels=val_labels,\n",
    "    mode='val',\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "# Test dataset without augmentation\n",
    "test_dataset = FashionMNISTDataset(\n",
    "    features=test_features,\n",
    "    labels=test_labels,\n",
    "    mode='test',\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "# Print class distributions\n",
    "print(\"\\nClass distributions:\")\n",
    "print(\"Training:\", train_dataset.get_class_distribution())\n",
    "print(\"Validation:\", val_dataset.get_class_distribution())\n",
    "print(\"Test:\", test_dataset.get_class_distribution())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders created:\n",
      "  Training: 796 batches of size 64\n",
      "  Validation: 71 batches of size 128\n",
      "  Test: 79 batches of size 128\n",
      "  Workers: 0, Pin Memory: True\n",
      "\n",
      "Testing DataLoaders:\n",
      "Training Batch 0:\n",
      "  Images shape: torch.Size([64, 1, 28, 28])\n",
      "  Labels shape: torch.Size([64])\n",
      "  Image range: [-1.000, 1.000]\n",
      "  Unique labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Training Batch 1:\n",
      "  Images shape: torch.Size([64, 1, 28, 28])\n",
      "  Labels shape: torch.Size([64])\n",
      "  Image range: [-1.000, 1.000]\n",
      "  Unique labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Training Batch 2:\n",
      "  Images shape: torch.Size([64, 1, 28, 28])\n",
      "  Labels shape: torch.Size([64])\n",
      "  Image range: [-1.000, 1.000]\n",
      "  Unique labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# Industrial DataLoader Configuration\n",
    "class DataLoaderManager:\n",
    "    \"\"\"Manager for creating optimized DataLoaders\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_dataloaders(train_dataset: Dataset, \n",
    "                         val_dataset: Dataset, \n",
    "                         test_dataset: Dataset,\n",
    "                         batch_size: int = 64,\n",
    "                         num_workers: int = None,\n",
    "                         pin_memory: bool = None) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "        \"\"\"\n",
    "        Create optimized DataLoaders with industrial settings\n",
    "        \n",
    "        Args:\n",
    "            train_dataset, val_dataset, test_dataset: Dataset objects\n",
    "            batch_size: Batch size for training\n",
    "            num_workers: Number of worker processes\n",
    "            pin_memory: Whether to pin memory for GPU transfer\n",
    "        \"\"\"\n",
    "        \n",
    "        # Auto-configure num_workers and pin_memory based on system\n",
    "        if num_workers is None:\n",
    "            num_workers = 0\n",
    "            # num_workers = min(4, os.cpu_count())  # Conservative default\n",
    "            \n",
    "        if pin_memory is None:\n",
    "            pin_memory = torch.cuda.is_available()\n",
    "        \n",
    "        # Training DataLoader - with shuffling\n",
    "        train_loader = DataLoader(\n",
    "            dataset=train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,  # Always shuffle training data\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=True,  # Drop incomplete batches for consistent batch norm\n",
    "            persistent_workers=num_workers > 0  # Keep workers alive\n",
    "        )\n",
    "        \n",
    "        # Validation DataLoader - no shuffling, potentially larger batch\n",
    "        val_loader = DataLoader(\n",
    "            dataset=val_dataset,\n",
    "            batch_size=batch_size * 2,  # Can use larger batch for inference\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=False\n",
    "        )\n",
    "        \n",
    "        # Test DataLoader - no shuffling, potentially larger batch\n",
    "        test_loader = DataLoader(\n",
    "            dataset=test_dataset,\n",
    "            batch_size=batch_size * 2,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=False\n",
    "        )\n",
    "        \n",
    "        print(f\"DataLoaders created:\")\n",
    "        print(f\"  Training: {len(train_loader)} batches of size {batch_size}\")\n",
    "        print(f\"  Validation: {len(val_loader)} batches of size {batch_size * 2}\")\n",
    "        print(f\"  Test: {len(test_loader)} batches of size {batch_size * 2}\")\n",
    "        print(f\"  Workers: {num_workers}, Pin Memory: {pin_memory}\")\n",
    "        \n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_loader, val_loader, test_loader = DataLoaderManager.create_dataloaders(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Test the DataLoaders\n",
    "print(\"\\nTesting DataLoaders:\")\n",
    "for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "    print(f\"Training Batch {batch_idx}:\")\n",
    "    print(f\"  Images shape: {images.shape}\")\n",
    "    print(f\"  Labels shape: {labels.shape}\")\n",
    "    print(f\"  Image range: [{images.min():.3f}, {images.max():.3f}]\")\n",
    "    print(f\"  Unique labels: {torch.unique(labels).tolist()}\")\n",
    "    \n",
    "    if batch_idx == 2:  # Just show first 3 batches\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created with 569,226 parameters\n",
      "Trainable parameters: 569,226\n"
     ]
    }
   ],
   "source": [
    "# Improved Model Architecture (addressing the Softmax issue)\n",
    "class ImprovedCustomModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Improved model architecture with better practices\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape: tuple = (1, 28, 28), num_classes: int = 10, dropout_rate: float = 0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Calculate flattened input size\n",
    "        self.input_size = np.prod(input_shape)\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1, end_dim=-1),\n",
    "            \n",
    "            # First hidden layer\n",
    "            nn.Linear(in_features=self.input_size, out_features=512),\n",
    "            nn.BatchNorm1d(num_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            \n",
    "            # Second hidden layer\n",
    "            nn.Linear(in_features=512, out_features=256),\n",
    "            nn.BatchNorm1d(num_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            \n",
    "            # Third hidden layer\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.BatchNorm1d(num_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            \n",
    "            # Output layer (NO SOFTMAX - handled by CrossEntropyLoss)\n",
    "            nn.Linear(in_features=128, out_features=num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights using He initialization for ReLU networks\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.BatchNorm1d):\n",
    "                nn.init.constant_(module.weight, 1)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Create improved model\n",
    "model = ImprovedCustomModel(\n",
    "    input_shape=(1, 28, 28),\n",
    "    num_classes=10,\n",
    "    dropout_rate=0.3\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 5 epochs...\n",
      "\n",
      "Epoch [1/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/796 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 796/796 [01:21<00:00,  9.71it/s, Loss=1.0447, Acc=60.51%]\n",
      "Validation: 100%|██████████| 71/71 [00:01<00:00, 37.97it/s, Loss=0.4312, Acc=77.94%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2805, Train Acc: 60.51%\n",
      "Val Loss: 0.5958, Val Acc: 77.94%\n",
      "New best validation accuracy: 77.94%\n",
      "\n",
      "Epoch [2/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 796/796 [01:21<00:00,  9.74it/s, Loss=0.7723, Acc=70.23%]\n",
      "Validation: 100%|██████████| 71/71 [00:01<00:00, 42.71it/s, Loss=0.3470, Acc=80.21%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8003, Train Acc: 70.23%\n",
      "Val Loss: 0.5259, Val Acc: 80.21%\n",
      "New best validation accuracy: 80.21%\n",
      "\n",
      "Epoch [3/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 796/796 [01:22<00:00,  9.62it/s, Loss=0.4924, Acc=73.51%]\n",
      "Validation: 100%|██████████| 71/71 [00:02<00:00, 34.99it/s, Loss=0.3450, Acc=81.53%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7048, Train Acc: 73.51%\n",
      "Val Loss: 0.4914, Val Acc: 81.53%\n",
      "New best validation accuracy: 81.53%\n",
      "\n",
      "Epoch [4/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 796/796 [01:22<00:00,  9.64it/s, Loss=0.7790, Acc=74.97%]\n",
      "Validation: 100%|██████████| 71/71 [00:01<00:00, 41.11it/s, Loss=0.3281, Acc=81.73%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6678, Train Acc: 74.97%\n",
      "Val Loss: 0.4837, Val Acc: 81.73%\n",
      "New best validation accuracy: 81.73%\n",
      "\n",
      "Epoch [5/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 796/796 [01:25<00:00,  9.32it/s, Loss=0.6228, Acc=76.05%]\n",
      "Validation: 100%|██████████| 71/71 [00:01<00:00, 38.84it/s, Loss=0.3145, Acc=81.59%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6411, Train Acc: 76.05%\n",
      "Val Loss: 0.4657, Val Acc: 81.59%\n",
      "\n",
      "Training completed in 423.69 seconds\n",
      "Best validation accuracy: 81.73%\n"
     ]
    }
   ],
   "source": [
    "# Industrial Training Loop with Validation\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"Industrial-grade training class with validation and monitoring\"\"\"\n",
    "    \n",
    "    def __init__(self, model, train_loader, val_loader, criterion, optimizer, device, scheduler=None):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.scheduler = scheduler\n",
    "        \n",
    "        # Metrics tracking\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "        \n",
    "    def train_epoch(self):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(self.train_loader, desc='Training')\n",
    "        for batch_idx, (images, labels) in enumerate(pbar):\n",
    "            # Move data to device\n",
    "            images, labels = images.to(self.device), labels.to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping (optional but good practice)\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "        \n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        accuracy = 100. * correct / total\n",
    "        \n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def validate_epoch(self):\n",
    "        \"\"\"Validate for one epoch\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(self.val_loader, desc='Validation')\n",
    "            for batch_idx, (images, labels) in enumerate(pbar):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                pbar.set_postfix({\n",
    "                    'Loss': f'{loss.item():.4f}',\n",
    "                    'Acc': f'{100.*correct/total:.2f}%'\n",
    "                })\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        accuracy = 100. * correct / total\n",
    "        \n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def train(self, epochs):\n",
    "        \"\"\"Complete training loop with validation\"\"\"\n",
    "        print(f\"Starting training for {epochs} epochs...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        best_val_acc = 0\n",
    "        patience = 5\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\nEpoch [{epoch+1}/{epochs}]\")\n",
    "            \n",
    "            # Training\n",
    "            train_loss, train_acc = self.train_epoch()\n",
    "            \n",
    "            # Validation\n",
    "            val_loss, val_acc = self.validate_epoch()\n",
    "            \n",
    "            # Update learning rate scheduler\n",
    "            if self.scheduler:\n",
    "                self.scheduler.step(val_loss)\n",
    "            \n",
    "            # Store metrics\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.train_accuracies.append(train_acc)\n",
    "            self.val_accuracies.append(val_acc)\n",
    "            \n",
    "            # Print epoch summary\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                patience_counter = 0\n",
    "                # Save best model\n",
    "                torch.save(self.model.state_dict(), 'best_model.pth')\n",
    "                print(f\"New best validation accuracy: {best_val_acc:.2f}%\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping after {epoch+1} epochs\")\n",
    "                break\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\nTraining completed in {total_time:.2f} seconds\")\n",
    "        print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "# Setup training components\n",
    "learning_rate = 0.001  # Reduced learning rate\n",
    "epochs = 5\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(  # AdamW is often better than Adam\n",
    "    model.parameters(), \n",
    "    lr=learning_rate,\n",
    "    weight_decay=1e-4  # L2 regularization\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.5, \n",
    "    patience=3\n",
    ")\n",
    "\n",
    "# Create trainer and start training\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    scheduler=scheduler\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train(epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "503fa3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 79/79 [00:03<00:00, 26.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results:\n",
      "Average Loss: 0.4792\n",
      "Accuracy: 81.81%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " T-shirt/top      0.728     0.817     0.770      1000\n",
      "     Trouser      0.924     0.971     0.947      1000\n",
      "    Pullover      0.746     0.660     0.700      1000\n",
      "       Dress      0.821     0.834     0.827      1000\n",
      "        Coat      0.675     0.871     0.760      1000\n",
      "      Sandal      0.895     0.904     0.900      1000\n",
      "       Shirt      0.625     0.384     0.476      1000\n",
      "     Sneaker      0.870     0.860     0.865      1000\n",
      "         Bag      0.946     0.954     0.950      1000\n",
      "  Ankle boot      0.912     0.926     0.919      1000\n",
      "\n",
      "    accuracy                          0.818     10000\n",
      "   macro avg      0.814     0.818     0.811     10000\n",
      "weighted avg      0.814     0.818     0.811     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Industrial Testing with Detailed Metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"Industrial-grade model evaluation class\"\"\"\n",
    "    \n",
    "    def __init__(self, model, test_loader, device, class_names):\n",
    "        self.model = model\n",
    "        self.test_loader = test_loader\n",
    "        self.device = device\n",
    "        self.class_names = class_names\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"Comprehensive model evaluation\"\"\"\n",
    "        self.model.eval()\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        total_loss = 0\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        print(\"Evaluating model on test set...\")\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(self.test_loader, desc='Testing'):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                outputs = self.model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = 100. * sum(p == l for p, l in zip(all_predictions, all_labels)) / len(all_labels)\n",
    "        avg_loss = total_loss / len(self.test_loader)\n",
    "        \n",
    "        print(f\"\\nTest Results:\")\n",
    "        print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "        print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "        \n",
    "        # Detailed classification report\n",
    "        print(\"\\nClassification Report:\")\n",
    "        report = classification_report(\n",
    "            all_labels, \n",
    "            all_predictions, \n",
    "            target_names=self.class_names,\n",
    "            digits=3\n",
    "        )\n",
    "        print(report)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'loss': avg_loss,\n",
    "            'predictions': all_predictions,\n",
    "            'labels': all_labels,\n",
    "            'report': report\n",
    "        }\n",
    "\n",
    "# Load best model and evaluate\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "evaluator = ModelEvaluator(\n",
    "    model=model,\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    class_names=data_manager.class_names\n",
    ")\n",
    "\n",
    "results = evaluator.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
